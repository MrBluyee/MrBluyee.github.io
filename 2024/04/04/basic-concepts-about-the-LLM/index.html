<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content=""><title>basic concepts about the LLM | Mr Bluyee's Blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement('script');
  hm.src = 'https://hm.baidu.com/hm.js?' + '3dbaf3043b34b1c894c4cd063629b6a9';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();</script><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"><meta name="generator" content="Hexo 7.1.1"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">basic concepts about the LLM</h1><a id="logo" href="/.">Mr Bluyee's Blog</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">basic concepts about the LLM</h1><div class="post-meta">2024-04-04<span> | </span><span class="category"><a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94AI/">学习笔记——AI</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 2.1k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 12</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#generative-AI"><span class="toc-number">1.</span> <span class="toc-text">generative AI</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#types-of-generative-AI-models"><span class="toc-number">1.1.</span> <span class="toc-text">types of generative AI models</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#what-is-a-language-model"><span class="toc-number">2.</span> <span class="toc-text">what is a language model</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#what-is-a-large-language-model"><span class="toc-number">3.</span> <span class="toc-text">what is a large language model</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#usage-of-the-LLM"><span class="toc-number">4.</span> <span class="toc-text">usage of the LLM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-LLM-work"><span class="toc-number">5.</span> <span class="toc-text">how LLM work</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#concepts-in-the-LLM"><span class="toc-number">6.</span> <span class="toc-text">concepts in the LLM</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Model-architecture"><span class="toc-number">6.1.</span> <span class="toc-text">Model architecture</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Model-size"><span class="toc-number">6.2.</span> <span class="toc-text">Model size</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#transformer"><span class="toc-number">6.3.</span> <span class="toc-text">transformer</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#attention-mechanism"><span class="toc-number">6.4.</span> <span class="toc-text">attention mechanism</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Self-attention"><span class="toc-number">6.5.</span> <span class="toc-text">Self-attention</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#multihead-attention"><span class="toc-number">6.6.</span> <span class="toc-text">multihead attention</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#parameters"><span class="toc-number">6.7.</span> <span class="toc-text">parameters</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Tokenization"><span class="toc-number">6.8.</span> <span class="toc-text">Tokenization</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#pre-training"><span class="toc-number">6.9.</span> <span class="toc-text">pre-training</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#prompt"><span class="toc-number">6.10.</span> <span class="toc-text">prompt</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#prompt-engineering"><span class="toc-number">6.11.</span> <span class="toc-text">prompt engineering</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Temperature"><span class="toc-number">6.12.</span> <span class="toc-text">Temperature</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#prompt-tuning"><span class="toc-number">6.13.</span> <span class="toc-text">prompt-tuning</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#fine-tuning"><span class="toc-number">6.14.</span> <span class="toc-text">fine-tuning</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Transfer-learning"><span class="toc-number">6.15.</span> <span class="toc-text">Transfer learning</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#reinforcement-learning-with-human-feedback-RLHF"><span class="toc-number">6.16.</span> <span class="toc-text">reinforcement learning with human feedback (RLHF)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#performance-of-the-LLM"><span class="toc-number">7.</span> <span class="toc-text">performance of the LLM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#White-Papers-for-Learning-LLMs"><span class="toc-number">8.</span> <span class="toc-text">White Papers for Learning LLMs</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#reference"><span class="toc-number">9.</span> <span class="toc-text">reference</span></a></li></ol></div></div><div class="post-content"><p>Large language models (LLMs) are advanced AI-based models trained to process and generate human language in a way that closely mirrors natural human communication. In a nutshell, LLMs are designed to understand and generate text like a human, in addition to other forms of content, based on the vast amount of data used to train them. They have the ability to infer from context, generate coherent and contextually relevant responses, translate to languages other than English, summarize text, answer questions (general conversation and FAQs) and even assist in creative writing or code generation tasks. </p>
<h3 id="generative-AI"><a href="#generative-AI" class="headerlink" title="generative AI"></a>generative AI</h3><p>Generative AI is a kind of machine learning techniques that involve the creation of new data samples from the trained models. These models can also be called as the generative models. In other words, generative models learn the underlying patterns and structures of a given dataset and can generate new samples that resemble the original data. </p>
<h4 id="types-of-generative-AI-models"><a href="#types-of-generative-AI-models" class="headerlink" title="types of generative AI models"></a>types of generative AI models</h4><p><em>- Generative Adversarial Networks (GANs):</em><br>These involve two neural networks, a generator and a discriminator, which work against each other to improve the quality of generated data, often used in realistic image generation.</p>
<p><em>- Variational Autoencoders (VAEs):</em><br>VAEs are used for generating new data points by learning a compressed representation of the input data, commonly applied in image processing and generation.</p>
<p><em>- Encoder-Decoder Transformer Architecture (e.g., T5, BART):</em><br>These models are designed for tasks like text translation, summarization, and question-answering, where both input and output are sequences of data.</p>
<p><em>- Encoder-Only Transformer Architecture (e.g., BERT):</em><br>Primarily used for understanding and processing input data, such as for language understanding tasks (LLMs), but not typically used for generative purposes like the other models mentioned.</p>
<p><em>- Autoregressive (Decoder-Only Transformer such as GPT):</em><br>These models predict the next item in a sequence, making them powerful for tasks like text generation (LLMs).</p>
<p><em>- Flow-Based Models:</em><br>These models, such as Normalizing Flows, are designed to explicitly model the distribution of data, allowing for both efficient generation and density estimation.</p>
<p><em>- Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) Networks:</em><br>Earlier used in sequence generation tasks like text and music before the rise of transformer models.</p>
<p><em>- Hybrid Models:</em><br>Some newer architectures combine elements of different models (like GANs and VAEs) to leverage the strengths of each in generating complex data.</p>
<p><em>LLMs are based on the Transformer Architecture</em>, which allows them to capture complex language patterns and relationships between words or phrases in large-scale text datasets.</p>
<h3 id="what-is-a-language-model"><a href="#what-is-a-language-model" class="headerlink" title="what is a language model"></a>what is a language model</h3><p>A language model is a machine learning model that aims to predict and generate plausible language. Autocomplete is a language model, for example.</p>
<p>These models work by estimating the probability of a token or sequence of tokens occurring within a longer sequence of tokens. Consider the following sentence:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">When I hear rain on my roof, I _______ in my kitchen.</span><br></pre></td></tr></table></figure>
<p>If you assume that a token is a word, then a language model determines the probabilities of different words or sequences of words to replace that underscore. For example, a language model might determine the following probabilities:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cook soup 9.4%</span><br><span class="line">warm up a kettle 5.2%</span><br><span class="line">cower 3.6%</span><br><span class="line">nap 2.5%</span><br><span class="line">relax 2.2%</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>A “sequence of tokens” could be an entire sentence or a series of sentences. That is, a language model could calculate the likelihood of different entire sentences or blocks of text.</p>
<p>Estimating the probability of what comes next in a sequence is useful for all kinds of things: generating text, translating languages, and answering questions, to name a few.</p>
<h3 id="what-is-a-large-language-model"><a href="#what-is-a-large-language-model" class="headerlink" title="what is a large language model"></a>what is a large language model</h3><p>Modeling human language at scale is a highly complex and resource-intensive endeavor. The path to reaching the current capabilities of language models and large language models has spanned several decades.</p>
<p>As models are built bigger and bigger, their complexity and efficacy increases. Early language models could predict the probability of a single word; modern large language models can predict the probability of sentences, paragraphs, or even entire documents.</p>
<p>The size and capability of language models has exploded over the last few years as computer memory, dataset size, and processing power increases, and more effective techniques for modeling longer text sequences are developed.</p>
<p>The large usually describes the num of parameters of the language model. Large language models (LLMs) are called “large” because they are pre-trained with a large number of parameters (100M+) on large corpora of text to process&#x2F;understand and generate natural language text for a wide variety of NLP tasks. </p>
<h3 id="usage-of-the-LLM"><a href="#usage-of-the-LLM" class="headerlink" title="usage of the LLM"></a>usage of the LLM</h3><p><em>Text generation</em><br>language generation abilities, such as writing emails, blog posts or other mid-to-long form content in response to prompts that can be refined and polished. </p>
<p><em>Content summarization</em><br>summarize long articles, news stories, research reports, corporate documentation and even customer history into thorough texts tailored in length to the output format.</p>
<p><em>AI assistants</em><br>chatbots that answer customer queries, perform backend tasks and provide detailed information in natural language as a part of an integrated, self-serve customer care solution. </p>
<p><em>Code generation</em><br>assists developers in building applications, finding errors in code and uncovering security issues in multiple programming languages, even “translating” between them.</p>
<p><em>Sentiment analysis</em><br>analyze text to determine the customer’s tone in order understand customer feedback at scale and aid in brand reputation management. </p>
<p><em>Language translation</em><br>provides wider coverage to organizations across languages and geographies with fluent translations and multilingual capabilities. </p>
<h3 id="how-LLM-work"><a href="#how-LLM-work" class="headerlink" title="how LLM work"></a>how LLM work</h3><p>LLMs operate by leveraging deep learning techniques and vast amounts of textual data. These models are typically based on a transformer architecture, like the generative pre-trained transformer, which excels at handling sequential data like text input. LLMs consist of multiple layers of neural networks, each with parameters that can be fine-tuned during training, which are enhanced further by a numerous layer known as the attention mechanism, which dials in on specific parts of data sets.</p>
<p>During the training process, these models learn to predict the next word in a sentence based on the context provided by the preceding words. The model does this through attributing a probability score to the recurrence of words that have been tokenized—broken down into smaller sequences of characters. These tokens are then transformed into embeddings, which are numeric representations of this context.</p>
<p>To ensure accuracy, this process involves training the LLM on a massive corpora of text (in the billions of pages), allowing it to learn grammar, semantics and conceptual relationships through zero-shot and self-supervised learning. Once trained on this training data, LLMs can generate text by autonomously predicting the next word based on the input they receive, and drawing on the patterns and knowledge they’ve acquired. The result is coherent and contextually relevant language generation that can be harnessed for a wide range of NLU and content generation tasks.</p>
<p>Model performance can also be increased through prompt engineering, prompt-tuning, fine-tuning and other tactics like reinforcement learning with human feedback (RLHF) to remove the biases, hateful speech and factually incorrect answers known as “hallucinations” that are often unwanted byproducts of training on so much unstructured data. This is one of the most important aspects of ensuring enterprise-grade LLMs are ready for use and do not expose organizations to unwanted liability, or cause damage to their reputation. </p>
<h3 id="concepts-in-the-LLM"><a href="#concepts-in-the-LLM" class="headerlink" title="concepts in the LLM"></a>concepts in the LLM</h3><h4 id="Model-architecture"><a href="#Model-architecture" class="headerlink" title="Model architecture"></a>Model architecture</h4><h4 id="Model-size"><a href="#Model-size" class="headerlink" title="Model size"></a>Model size</h4><h4 id="transformer"><a href="#transformer" class="headerlink" title="transformer"></a>transformer</h4><h4 id="attention-mechanism"><a href="#attention-mechanism" class="headerlink" title="attention mechanism"></a>attention mechanism</h4><p>Attention mechanisms in LLMs, particularly the self-attention mechanism used in transformers, allow the model to weigh the importance of different words or phrases in a given context. By assigning different weights to the tokens in the input sequence, the model can focus on the most relevant information while ignoring less important details. This ability to selectively focus on specific parts of the input is crucial for capturing long-range dependencies and understanding the nuances of natural language.</p>
<h4 id="Self-attention"><a href="#Self-attention" class="headerlink" title="Self-attention"></a>Self-attention</h4><h4 id="multihead-attention"><a href="#multihead-attention" class="headerlink" title="multihead attention"></a>multihead attention</h4><h4 id="parameters"><a href="#parameters" class="headerlink" title="parameters"></a>parameters</h4><p>LLMs have millions or even billions of parameters, each influencing how the model comprehends language. These parameters can include:</p>
<ul>
<li><p>Weights: These determine the importance of specific connections between words and phrases, allowing the model to learn patterns and relationships. Weights are numerical values that define the strength of connections between neurons across different layers in the model. In the context of LLMs, weights are primarily used in the attention mechanism and the feedforward neural networks that make up the model’s architecture. They are adjusted during the training process to optimize the model’s ability to generate relevant and coherent text.</p>
</li>
<li><p>Biases: These act as starting points, guiding the model’s interpretations before it sees data. Biases are additional numerical values that are added to the weighted sum of inputs before being passed through an activation function. They help to control the output of neurons and provide flexibility in the model’s learning process. Biases can be thought of as a way to shift the activation function to the left or right, allowing the model to learn more complex patterns and relationships in the input data.</p>
</li>
<li><p>Embedding vectors: These represent words numerically, enabling the model to understand their meaning and context.</p>
</li>
</ul>
<h4 id="Tokenization"><a href="#Tokenization" class="headerlink" title="Tokenization"></a>Tokenization</h4><p>Tokenization is the process of converting a sequence of text into individual words, subwords, or tokens that the model can understand. In LLMs, tokenization is usually performed using subword algorithms like Byte Pair Encoding (BPE) or WordPiece, which split the text into smaller units that capture both frequent and rare words. This approach helps to limit the model’s vocabulary size while maintaining its ability to represent any text sequence.</p>
<h4 id="pre-training"><a href="#pre-training" class="headerlink" title="pre-training"></a>pre-training</h4><p>Pretraining is the process of training an LLM on a large dataset, usually unsupervised or self-supervised, before fine-tuning it for a specific task. During pretraining, the model learns general language patterns, relationships between words, and other foundational knowledge. This process results in a pre-trained model that can be fine-tuned using a smaller, task-specific dataset, significantly reducing the amount of labeled data and training time required to achieve high performance on various NLP tasks.</p>
<h4 id="prompt"><a href="#prompt" class="headerlink" title="prompt"></a>prompt</h4><h4 id="prompt-engineering"><a href="#prompt-engineering" class="headerlink" title="prompt engineering"></a>prompt engineering</h4><h4 id="Temperature"><a href="#Temperature" class="headerlink" title="Temperature"></a>Temperature</h4><p>The LLM temperature is a hyperparameter that regulates the randomness, or creativity, of the AI’s responses. It determines how creative the model should be. It has probabilities for all the different words that could follow and then selects the next word to output. The Temperature setting tells it which of these words it can use. A Temperature of 0 makes the model deterministic. It limits the model to use the word with the highest probability. You can run it over and over and get the same output. As you increase the Temperature, the limit softens, allowing it to use words with lower and lower probabilities.</p>
<h4 id="prompt-tuning"><a href="#prompt-tuning" class="headerlink" title="prompt-tuning"></a>prompt-tuning</h4><h4 id="fine-tuning"><a href="#fine-tuning" class="headerlink" title="fine-tuning"></a>fine-tuning</h4><h4 id="Transfer-learning"><a href="#Transfer-learning" class="headerlink" title="Transfer learning"></a>Transfer learning</h4><p>Transfer learning is the technique of leveraging the knowledge gained during pretraining and applying it to a new, related task. In the context of LLMs, transfer learning involves fine-tuning a pre-trained model on a smaller, task-specific dataset to achieve high performance on that task. The benefit of transfer learning is that it allows the model to benefit from the vast amount of general language knowledge learned during pre-training, reducing the need for large labeled datasets and extensive training for each new task.</p>
<h4 id="reinforcement-learning-with-human-feedback-RLHF"><a href="#reinforcement-learning-with-human-feedback-RLHF" class="headerlink" title="reinforcement learning with human feedback (RLHF)"></a>reinforcement learning with human feedback (RLHF)</h4><h3 id="performance-of-the-LLM"><a href="#performance-of-the-LLM" class="headerlink" title="performance of the LLM"></a>performance of the LLM</h3><h3 id="White-Papers-for-Learning-LLMs"><a href="#White-Papers-for-Learning-LLMs" class="headerlink" title="White Papers for Learning LLMs"></a>White Papers for Learning LLMs</h3><p>Neural Machine Translation by Jointly Learning to Align and Translate (2014)<br>by Bahdanau, Cho, and Bengio,<br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.0473">https://arxiv.org/abs/1409.0473</a></p>
<p>Attention Is All You Need (2017)<br>by Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin,<br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a></p>
<p>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2018)<br>by Devlin, Chang, Lee, and Toutanova,<br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.04805">https://arxiv.org/abs/1810.04805</a></p>
<p>Improving Language Understanding by Generative Pre-Training (2018)<br>by Radford and Narasimhan,<br><a target="_blank" rel="noopener" href="https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035">https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035</a></p>
<p>BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension (2019)<br>by Lewis, Liu, Goyal, Ghazvininejad, Mohamed, Levy, Stoyanov, and Zettlemoyer,<br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1910.13461">https://arxiv.org/abs/1910.13461</a></p>
<p>Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond (2023)<br>by Yang, Jin, Tang, Han, Feng, Jiang, Yin, and Hu,<br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2304.13712">https://arxiv.org/abs/2304.13712</a></p>
<h3 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h3><p><a target="_blank" rel="noopener" href="https://vitalflux.com/generative-modeling-in-machine-learning-examples/">https://vitalflux.com/generative-modeling-in-machine-learning-examples/</a><br><a target="_blank" rel="noopener" href="https://www.superannotate.com/blog/llm-overview">https://www.superannotate.com/blog/llm-overview</a><br><a target="_blank" rel="noopener" href="https://vitalflux.com/large-language-models-concepts-examples/">https://vitalflux.com/large-language-models-concepts-examples/</a><br><a target="_blank" rel="noopener" href="https://www.ibm.com/topics/large-language-models">https://www.ibm.com/topics/large-language-models</a><br><a target="_blank" rel="noopener" href="https://developers.google.com/machine-learning/resources/intro-llms">https://developers.google.com/machine-learning/resources/intro-llms</a><br><a target="_blank" rel="noopener" href="https://txt.cohere.com/llm-parameters-best-outputs-language-ai/">https://txt.cohere.com/llm-parameters-best-outputs-language-ai/</a><br><a target="_blank" rel="noopener" href="https://www.thecloudgirl.dev/blog/llm-parameters-explained">https://www.thecloudgirl.dev/blog/llm-parameters-explained</a><br><a target="_blank" rel="noopener" href="https://deepchecks.com/glossary/llm-parameters/">https://deepchecks.com/glossary/llm-parameters/</a><br><a target="_blank" rel="noopener" href="https://datascience.stackexchange.com/questions/120764/how-does-an-llm-parameter-relate-to-a-weight-in-a-neural-network">https://datascience.stackexchange.com/questions/120764/how-does-an-llm-parameter-relate-to-a-weight-in-a-neural-network</a></p>
</div><div class="post-copyright"><script type="text/javascript" src="/js/copyright.js?v=1.0.0" successtext="复制成功！"></script><link rel="stylesheet" type="text/css" href="/css/copyright.css?v=1.0.0"><p><span>本文标题：</span>basic concepts about the LLM</p><p><span>文章作者：</span>Mr Bluyee</p><p><span>发布时间：</span>2024-04-04</p><p><span>最后更新：</span>2024-04-07</p><p><span>原始链接：</span><a href="/2024/04/04/basic-concepts-about-the-LLM/">https://www.mrbluyee.com/2024/04/04/basic-concepts-about-the-LLM/</a><span class="copy-path"><i class="fa fa-clipboard" data-clipboard-text="https://www.mrbluyee.com/2024/04/04/basic-concepts-about-the-LLM/"></i></span></p><p><span>版权声明：</span>The author owns the copyright, please indicate the source reproduced.</p></div><br><div class="tags"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/" rel="tag">AI</a></li></ul></div><div class="post-nav"><a class="pre" href="/2024/04/07/Retrieval-Augmented-Generation-RAG/">Retrieval Augmented Generation (RAG)</a><a class="next" href="/2024/04/02/anything-LLM-with-ollama-a-private-document-chatbot/">anything LLM with ollama - a private document chatbot</a></div><div id="lv-container" data-id="city" data-uid="MTAyMC8zOTYxNS8xNjE0Mg"><script>(function(d, s) {
  var j, e = d.getElementsByTagName(s)[0];
  if (typeof LivereTower === 'function') { return; }
  j = d.createElement(s);
  j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
  j.async = true;
  e.parentNode.insertBefore(j, e);
})(document, 'script');
</script></div></div></div></div><div class="pure-u-1 pure-u-md-1-4"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"/><div id="local-search-result"></div></div></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img class="nofancybox" src="/img/avatar.jpeg"/></a><p>Enjoy life / Hope for future</p><a class="info-icon" href="mr.bluyee@hotmail.com" title="mr.bluyee@hotmail.com" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/MrBluyee" title="https://github.com/MrBluyee" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94AI/">学习笔记——AI</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94C-%E7%AE%97%E6%B3%95/">学习笔记——C 算法</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94C%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">学习笔记——C数据结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94Python/">学习笔记——Python</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94android/">学习笔记——android</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94data-compression/">学习笔记——data compression</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94expert-c-programming/">学习笔记——expert c programming</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94linux/">学习笔记——linux</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94opencv/">学习笔记——opencv</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E4%BB%A3%E7%A0%81%E7%AE%A1%E7%90%86/">学习笔记——代码管理</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E4%BD%8E%E5%8A%9F%E8%80%97%E6%8A%80%E6%9C%AF/">学习笔记——低功耗技术</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%BC%80%E5%8F%91/">学习笔记——嵌入式开发</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">学习笔记——机器学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E7%89%A9%E8%81%94%E7%BD%91/">学习笔记——物联网</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/">学习笔记——网络协议</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F/">学习笔记——计算机系统</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F/" style="font-size: 15px;">计算机系统</a> <a href="/tags/android/" style="font-size: 15px;">android</a> <a href="/tags/C/" style="font-size: 15px;">C</a> <a href="/tags/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/" style="font-size: 15px;">网络协议</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/linux/" style="font-size: 15px;">linux</a> <a href="/tags/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%BC%80%E5%8F%91/" style="font-size: 15px;">嵌入式开发</a> <a href="/tags/AI/" style="font-size: 15px;">AI</a> <a href="/tags/data-compression/" style="font-size: 15px;">data compression</a> <a href="/tags/opencv/" style="font-size: 15px;">opencv</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">机器学习</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2024/04/07/Retrieval-Augmented-Generation-RAG/">Retrieval Augmented Generation (RAG)</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/04/04/basic-concepts-about-the-LLM/">basic concepts about the LLM</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/04/02/anything-LLM-with-ollama-a-private-document-chatbot/">anything LLM with ollama - a private document chatbot</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/04/02/open-webui-with-ollama-ChatGPT-like-interface/">open-webui with ollama - ChatGPT like interface</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/04/02/ollama-run-large-language-models-locally/">ollama - run large language models locally</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/03/28/huffman-coding-in-deflate/">huffman coding in deflate</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/03/25/zlib-and-gzip-format/">zlib and gzip format</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/03/23/deflate%20format/">deflate format</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/09/23/IPsec/">IPsec</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/07/21/%E6%95%B0%E7%9A%84%E7%B4%A0%E6%80%A7%E6%B5%8B%E8%AF%95/">数的素性测试</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2024 <a href="/." rel="nofollow">Mr Bluyee's Blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0"></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><link rel="stylesheet" type="text/css" href="/css/search.css?v=1.0.0"><script type="text/javascript" src="/js/search.js?v=1.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
  search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="复制成功！"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>