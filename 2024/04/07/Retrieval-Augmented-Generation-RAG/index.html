<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content=""><title>Retrieval Augmented Generation (RAG) | Mr Bluyee's Blog</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement('script');
  hm.src = 'https://hm.baidu.com/hm.js?' + '3dbaf3043b34b1c894c4cd063629b6a9';
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();</script><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"><meta name="generator" content="Hexo 7.1.1"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Retrieval Augmented Generation (RAG)</h1><a id="logo" href="/.">Mr Bluyee's Blog</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Retrieval Augmented Generation (RAG)</h1><div class="post-meta">2024-04-07<span> | </span><span class="category"><a href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94AI/">学习笔记——AI</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 1.9k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-clock-o"></i><span class="post-count"> 11</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#How-RAG-work"><span class="toc-number">1.</span> <span class="toc-text">How RAG work</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Ingestion-Stage-indexing"><span class="toc-number">1.1.</span> <span class="toc-text">Ingestion Stage - indexing</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Inferencing-Stage-Retrieval-Generation"><span class="toc-number">1.2.</span> <span class="toc-text">Inferencing Stage - Retrieval &amp; Generation</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Frameworks-with-RAG"><span class="toc-number">2.</span> <span class="toc-text">Frameworks with RAG</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#LlamaIndex"><span class="toc-number">2.1.</span> <span class="toc-text">LlamaIndex</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#LlamaIndex-concepts"><span class="toc-number">2.1.1.</span> <span class="toc-text">LlamaIndex concepts</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LangChain"><span class="toc-number">2.2.</span> <span class="toc-text">LangChain</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#reference"><span class="toc-number">3.</span> <span class="toc-text">reference</span></a></li></ol></div></div><div class="post-content"><p>Retrieval-augmented generation (RAG) is an AI framework for improving the quality of LLM-generated responses by grounding the model on external sources of knowledge to supplement the LLM’s internal representation of information. Implementing RAG in an LLM-based question answering system has two main benefits: It ensures that the model has access to the most current, reliable facts, and that users have access to the model’s sources, ensuring that its claims can be checked for accuracy and ultimately trusted.</p>
<p>RAG has additional benefits. By grounding an LLM on a set of external, verifiable facts, the model has fewer opportunities to pull information baked into its parameters. This reduces the chances that an LLM will leak sensitive data, or ‘hallucinate’ incorrect or misleading information.</p>
<p>RAG also reduces the need for users to continuously train the model on new data and update its parameters as circumstances evolve. In this way, RAG can lower the computational and financial costs of running LLM-powered chatbots in an enterprise setting. </p>
<h2 id="How-RAG-work"><a href="#How-RAG-work" class="headerlink" title="How RAG work"></a>How RAG work</h2><p><img src="/images/retrieval_augmented_generation/basic_rag.png" alt="How RAG work"></p>
<p>In RAG, your data is loaded and prepared for queries or “indexed”. User queries act on the index, which filters your data down to the most relevant context. This context and your query then go to the LLM along with a prompt, and the LLM provides a response.</p>
<h3 id="Ingestion-Stage-indexing"><a href="#Ingestion-Stage-indexing" class="headerlink" title="Ingestion Stage - indexing"></a>Ingestion Stage - indexing</h3><p><img src="/images/retrieval_augmented_generation/indexing.png" alt="Ingestion Stage - indexing"></p>
<ul>
<li><p>Source Data Preparation<br>The starting point of any RAG system is its source data, often consisting of a vast corpus of text documents, websites, or databases. This data serves as the knowledge reservoir that the retrieval model scans through to find relevant information. </p>
</li>
<li><p>Data Loading: this refers to getting your data from where it lives – whether it’s text files, PDFs, another website, a database, or an API – into your pipeline.</p>
</li>
<li><p>Data Cleaning<br>It’s crucial to have diverse, accurate, and high-quality source data for optimal functioning. It is also important to manage and reduce redundancy in the source data.</p>
</li>
<li><p>Data Chunking<br>Before the retrieval model can search through the data, it’s typically divided into manageable “chunks” or segments. This chunking process ensures that the system can efficiently scan through the data and enables quick retrieval of relevant content. Effective chunking strategies can drastically improve the model’s speed and accuracy: a document may be its own chunk, but it could also be split up into chapters&#x2F;sections, paragraphs, sentences, or even just “chunks of words.” Remember: the goal is to be able to feed the Generative Model with information that will enhance its generation.</p>
</li>
<li><p>Text-to-Vector Conversion (Embedding Models)<br>The next step involves converting the textual data into a format that the model can readily use. When using a vector database, this means transforming the text into mathematical vectors via a process known as “embedding”. These are almost always generated using complex software models that have been built with machine learning techniques. These vectors encapsulate the semantics and context of the text, making it easier for the retrieval model to identify relevant data points. Many embedding models can be fine-tuned to create good semantic matching; general-purpose embedding models such as GPT and LLaMa may not perform as well against scientific information as a model like SciBERT, for example.</p>
</li>
<li><p>Vector Storing (Vector Database)<br>Once your data is indexed you will almost always want to store your index, as well as other metadata, to avoid having to re-index it. A vector database is designed to store, manage and index massive quantities of high-dimensional vector data efficiently. Unlike traditional relational databases with rows and columns, data points in a vector database are represented by vectors with a fixed number of dimensions, clustered based on similarity. This design enables low latency queries, making them ideal for AI-driven applications.</p>
</li>
</ul>
<h3 id="Inferencing-Stage-Retrieval-Generation"><a href="#Inferencing-Stage-Retrieval-Generation" class="headerlink" title="Inferencing Stage - Retrieval &amp; Generation"></a>Inferencing Stage - Retrieval &amp; Generation</h3><p><img src="/images/retrieval_augmented_generation/retrieval_generation.png" alt="Retrieval &amp; Generation"></p>
<p>In a RAG pattern, queries and responses are coordinated between the search engine and the LLM. A user’s question or query is forwarded to both the search engine and to the LLM as a prompt. The search results come back from the search engine and are redirected to an LLM. The response that makes it back to the user is generative AI, either a summation or answer from the LLM.</p>
<ul>
<li><p>Retrieve Models (should use the same model during embedding)<br>Retrieval models act as the information gatekeepers in the RAG architecture. Their primary function is to search through a large corpus of data to find relevant pieces of information that can be used for text generation. Think of them as specialized librarians who know exactly which ‘books’ to pull off the ‘shelves’ when you ask a question. These models use algorithms to rank and select the most pertinent data, offering a way to introduce external knowledge into the text generation process. By doing so, retrieval models set the stage for more informed, context-rich language generation, elevating the capabilities of traditional language models.</p>
</li>
<li><p>Generate Models<br>Once the retrieval model has sourced the appropriate information, generative models come into play. These models act as creative writers, synthesizing the retrieved information into coherent and contextually relevant text. Usually built upon Large Language Models (LLMs), generative models can create text that is grammatically correct, semantically meaningful, and aligned with the initial query or prompt. They take the raw data selected by the retrieval models and give it a narrative structure, making the information easily digestible and actionable. In the RAG framework, generative models serve as the final piece of the puzzle, providing the textual output we interact with.</p>
</li>
</ul>
<p>The Format of the prompt could be like:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def question_answering(context, query):</span><br><span class="line">    prompt = f&quot;&quot;&quot;</span><br><span class="line">                Give the answer to the user query delimited by triple backticks ```&#123;query&#125;```\</span><br><span class="line">                using the information given in context delimited by triple backticks ```&#123;context&#125;```.\</span><br><span class="line">                If there is no relevant information in the provided context, try to answer yourself, </span><br><span class="line">                but tell user that you did not have any relevant context to base your answer on.</span><br><span class="line">                Be concise and output the answer of size less than 80 tokens.</span><br><span class="line">                &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    response = get_completion(instruction, prompt, model=&quot;gpt-3.5-turbo&quot;)</span><br><span class="line">    answer = response.choices[0].message[&quot;content&quot;]</span><br><span class="line">    return answer</span><br></pre></td></tr></table></figure>

<h2 id="Frameworks-with-RAG"><a href="#Frameworks-with-RAG" class="headerlink" title="Frameworks with RAG"></a>Frameworks with RAG</h2><h3 id="LlamaIndex"><a href="#LlamaIndex" class="headerlink" title="LlamaIndex"></a>LlamaIndex</h3><p>LlamaIndex is a data framework for LLM-based applications which benefit from context augmentation. Such LLM systems have been termed as RAG systems, standing for “Retrieval-Augemented Generation”.<br>LlamaIndex provides the essential abstractions to more easily ingest, structure, and access private or domain-specific data in order to inject these safely and reliably into LLMs for more accurate text generation.</p>
<p>LlamaIndex provides the following tools to help you quickly standup production-ready RAG systems:</p>
<ul>
<li><p><em>Data connectors</em> ingest your existing data from their native source and format. These could be APIs, PDFs, SQL, and (much) more.</p>
</li>
<li><p><em>Data indexes</em> structure your data in intermediate representations that are easy and performant for LLMs to consume.</p>
</li>
<li><p><em>Engines</em> provide natural language access to your data. For example:</p>
<ul>
<li><p>Query engines are powerful retrieval interfaces for knowledge-augmented output.</p>
</li>
<li><p>Chat engines are conversational interfaces for multi-message, “back and forth” interactions with your data.</p>
</li>
</ul>
</li>
<li><p><em>Data agents</em> are LLM-powered knowledge workers augmented by tools, from simple helper functions to API integrations and more.</p>
</li>
<li><p><em>Application integrations</em> tie LlamaIndex back into the rest of your ecosystem. This could be LangChain, Flask, Docker, ChatGPT, or… anything else!</p>
</li>
</ul>
<h4 id="LlamaIndex-concepts"><a href="#LlamaIndex-concepts" class="headerlink" title="LlamaIndex concepts"></a>LlamaIndex concepts</h4><p>There are also some terms you’ll encounter that refer to steps within each of these stages.</p>
<p><em>Loading stage</em></p>
<ul>
<li><p>Nodes and Documents: A Document is a container around any data source - for instance, a PDF, an API output, or retrieve data from a database. A Node is the atomic unit of data in LlamaIndex and represents a “chunk” of a source Document. Nodes have metadata that relate them to the document they are in and to other nodes.</p>
</li>
<li><p>Connectors: A data connector (often called a Reader) ingests data from different data sources and data formats into Documents and Nodes.</p>
</li>
</ul>
<p><em>Indexing Stage</em></p>
<ul>
<li><p>Indexes: Once you’ve ingested your data, LlamaIndex will help you index the data into a structure that’s easy to retrieve. This usually involves generating vector embeddings which are stored in a specialized database called a vector store. Indexes can also store a variety of metadata about your data.</p>
</li>
<li><p>Embeddings LLMs generate numerical representations of data called embeddings. When filtering your data for relevance, LlamaIndex will convert queries into embeddings, and your vector store will find data that is numerically similar to the embedding of your query.</p>
</li>
</ul>
<p><em>Querying Stage</em></p>
<ul>
<li><p>Retrievers: A retriever defines how to efficiently retrieve relevant context from an index when given a query. Your retrieval strategy is key to the relevancy of the data retrieved and the efficiency with which it’s done.</p>
</li>
<li><p>Routers: A router determines which retriever will be used to retrieve relevant context from the knowledge base. More specifically, the RouterRetriever class, is responsible for selecting one or multiple candidate retrievers to execute a query. They use a selector to choose the best option based on each candidate’s metadata and the query.</p>
</li>
<li><p>Node Postprocessors: A node postprocessor takes in a set of retrieved nodes and applies transformations, filtering, or re-ranking logic to them.</p>
</li>
<li><p>Response Synthesizers: A response synthesizer generates a response from an LLM, using a user query and a given set of retrieved text chunks.</p>
</li>
</ul>
<p><em>Putting it all together</em><br>There are endless use cases for data-backed LLM applications but they can be roughly grouped into three categories:</p>
<ul>
<li><p>Query Engines: A query engine is an end-to-end pipeline that allows you to ask questions over your data. It takes in a natural language query, and returns a response, along with reference context retrieved and passed to the LLM.</p>
</li>
<li><p>Chat Engines: A chat engine is an end-to-end pipeline for having a conversation with your data (multiple back-and-forth instead of a single question-and-answer).</p>
</li>
<li><p>Agents: An agent is an automated decision-maker powered by an LLM that interacts with the world via a set of tools. Agents can take an arbitrary number of steps to complete a given task, dynamically deciding on the best course of action rather than following pre-determined steps. This gives it additional flexibility to tackle more complex tasks.</p>
</li>
</ul>
<h3 id="LangChain"><a href="#LangChain" class="headerlink" title="LangChain"></a>LangChain</h3><p>LangChain provides standard, extendable interfaces and external integrations:</p>
<ul>
<li><p>Retrieval<br>Interface with application-specific data for e.g. RAG</p>
</li>
<li><p>Document loaders<br>Load data from a source as Documents for later processing</p>
</li>
<li><p>Text splitters<br>Transform source documents to better suit your application</p>
</li>
<li><p>Embedding models<br>Create vector representations of a piece of text, allowing for natural language search</p>
</li>
<li><p>Vectorstores<br>Interfaces for specialized databases that can search over unstructured data with natural language</p>
</li>
<li><p>Retrievers<br>More generic interfaces that return documents given an unstructured query</p>
</li>
</ul>
<p>more details, see:<br><a target="_blank" rel="noopener" href="https://python.langchain.com/docs/modules/">https://python.langchain.com/docs/modules/</a><br><a target="_blank" rel="noopener" href="https://python.langchain.com/docs/use_cases/question_answering/quickstart/">https://python.langchain.com/docs/use_cases/question_answering/quickstart/</a></p>
<h2 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2005.11401v4.pdf">https://arxiv.org/pdf/2005.11401v4.pdf</a><br><a target="_blank" rel="noopener" href="https://research.ibm.com/blog/retrieval-augmented-generation-RAG">https://research.ibm.com/blog/retrieval-augmented-generation-RAG</a><br><a target="_blank" rel="noopener" href="https://www.ibm.com/topics/vector-database">https://www.ibm.com/topics/vector-database</a><br><a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/getting_started/concepts/">https://docs.llamaindex.ai/en/stable/getting_started/concepts/</a><br><a target="_blank" rel="noopener" href="https://docs.llamaindex.ai/en/stable/">https://docs.llamaindex.ai/en/stable/</a><br><a target="_blank" rel="noopener" href="https://www.datastax.com/guides/what-is-retrieval-augmented-generation">https://www.datastax.com/guides/what-is-retrieval-augmented-generation</a><br><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/rag-101-retrieval-augmented-generation-questions-answered/">https://developer.nvidia.com/blog/rag-101-retrieval-augmented-generation-questions-answered/</a><br><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/search/retrieval-augmented-generation-overview">https://learn.microsoft.com/en-us/azure/search/retrieval-augmented-generation-overview</a><br><a target="_blank" rel="noopener" href="https://python.langchain.com/docs/use_cases/question_answering/quickstart/">https://python.langchain.com/docs/use_cases/question_answering/quickstart/</a></p>
</div><div class="post-copyright"><script type="text/javascript" src="/js/copyright.js?v=1.0.0" successtext="复制成功！"></script><link rel="stylesheet" type="text/css" href="/css/copyright.css?v=1.0.0"><p><span>本文标题：</span>Retrieval Augmented Generation (RAG)</p><p><span>文章作者：</span>Mr Bluyee</p><p><span>发布时间：</span>2024-04-07</p><p><span>最后更新：</span>2024-04-07</p><p><span>原始链接：</span><a href="/2024/04/07/Retrieval-Augmented-Generation-RAG/">https://www.mrbluyee.com/2024/04/07/Retrieval-Augmented-Generation-RAG/</a><span class="copy-path"><i class="fa fa-clipboard" data-clipboard-text="https://www.mrbluyee.com/2024/04/07/Retrieval-Augmented-Generation-RAG/"></i></span></p><p><span>版权声明：</span>The author owns the copyright, please indicate the source reproduced.</p></div><br><div class="tags"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/" rel="tag">AI</a></li></ul></div><div class="post-nav"><a class="next" href="/2024/04/04/basic-concepts-about-the-LLM/">basic concepts about the LLM</a></div><div id="lv-container" data-id="city" data-uid="MTAyMC8zOTYxNS8xNjE0Mg"><script>(function(d, s) {
  var j, e = d.getElementsByTagName(s)[0];
  if (typeof LivereTower === 'function') { return; }
  j = d.createElement(s);
  j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
  j.async = true;
  e.parentNode.insertBefore(j, e);
})(document, 'script');
</script></div></div></div></div><div class="pure-u-1 pure-u-md-1-4"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"/><div id="local-search-result"></div></div></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="关于"><img class="nofancybox" src="/img/avatar.jpeg"/></a><p>Enjoy life / Hope for future</p><a class="info-icon" href="mr.bluyee@hotmail.com" title="mr.bluyee@hotmail.com" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/MrBluyee" title="https://github.com/MrBluyee" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94AI/">学习笔记——AI</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94C-%E7%AE%97%E6%B3%95/">学习笔记——C 算法</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94C%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">学习笔记——C数据结构</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94Python/">学习笔记——Python</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94android/">学习笔记——android</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94data-compression/">学习笔记——data compression</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94expert-c-programming/">学习笔记——expert c programming</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94linux/">学习笔记——linux</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94opencv/">学习笔记——opencv</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E4%BB%A3%E7%A0%81%E7%AE%A1%E7%90%86/">学习笔记——代码管理</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E4%BD%8E%E5%8A%9F%E8%80%97%E6%8A%80%E6%9C%AF/">学习笔记——低功耗技术</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%BC%80%E5%8F%91/">学习笔记——嵌入式开发</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">学习笔记——机器学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E7%89%A9%E8%81%94%E7%BD%91/">学习笔记——物联网</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/">学习笔记——网络协议</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F/">学习笔记——计算机系统</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F/" style="font-size: 15px;">计算机系统</a> <a href="/tags/android/" style="font-size: 15px;">android</a> <a href="/tags/C/" style="font-size: 15px;">C</a> <a href="/tags/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/" style="font-size: 15px;">网络协议</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/linux/" style="font-size: 15px;">linux</a> <a href="/tags/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%BC%80%E5%8F%91/" style="font-size: 15px;">嵌入式开发</a> <a href="/tags/AI/" style="font-size: 15px;">AI</a> <a href="/tags/data-compression/" style="font-size: 15px;">data compression</a> <a href="/tags/opencv/" style="font-size: 15px;">opencv</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">机器学习</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2024/04/07/Retrieval-Augmented-Generation-RAG/">Retrieval Augmented Generation (RAG)</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/04/04/basic-concepts-about-the-LLM/">basic concepts about the LLM</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/04/02/anything-LLM-with-ollama-a-private-document-chatbot/">anything LLM with ollama - a private document chatbot</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/04/02/open-webui-with-ollama-ChatGPT-like-interface/">open-webui with ollama - ChatGPT like interface</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/04/02/ollama-run-large-language-models-locally/">ollama - run large language models locally</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/03/28/huffman-coding-in-deflate/">huffman coding in deflate</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/03/25/zlib-and-gzip-format/">zlib and gzip format</a></li><li class="post-list-item"><a class="post-list-link" href="/2024/03/23/deflate%20format/">deflate format</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/09/23/IPsec/">IPsec</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/07/21/%E6%95%B0%E7%9A%84%E7%B4%A0%E6%80%A7%E6%B5%8B%E8%AF%95/">数的素性测试</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2024 <a href="/." rel="nofollow">Mr Bluyee's Blog.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js"></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0"></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><link rel="stylesheet" type="text/css" href="/css/search.css?v=1.0.0"><script type="text/javascript" src="/js/search.js?v=1.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
  search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="复制成功！"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>